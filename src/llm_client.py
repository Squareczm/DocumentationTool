"""
LLMå®¢æˆ·ç«¯
è´Ÿè´£ä¸å¤§è¯­è¨€æ¨¡å‹APIè¿›è¡Œäº¤äº’ï¼Œæå–æ–‡æ¡£ä¸»ä½“ã€å»ºè®®æ–‡ä»¶å¤¹ç­‰
"""

import json
import logging
import requests
from typing import Dict, Any, Optional, List
from datetime import datetime
import re
import os
from pathlib import Path
import yaml


class LLMClient:
    """å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            config: åŒ…å«LLMé…ç½®çš„å­—å…¸
        """
        self.config = config.get('llm', {})
        self.enabled = self.config.get('enabled', True)  # é»˜è®¤å¯ç”¨
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½åˆ†ç±»è§„åˆ™é…ç½®
        self.classification_rules = self._load_classification_rules()
        
        # APIå¯†é’¥è·å–ä¼˜å…ˆçº§ï¼šconfig.yaml > ç¯å¢ƒå˜é‡
        api_key = self.config.get('api_key')
        if not api_key:
            api_key = os.getenv('SMARTFILEORG_LLM_API_KEY')
        
        if not api_key:
            self.logger.warning("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·åœ¨config.yamlä¸­é…ç½®api_keyæˆ–è®¾ç½®ç¯å¢ƒå˜é‡SMARTFILEORG_LLM_API_KEY")
            self.enabled = False
        else:
            self.config['api_key'] = api_key
            self.logger.info(f"LLMå®¢æˆ·ç«¯å·²å¯ç”¨ï¼Œä½¿ç”¨æä¾›å•†: {self.config.get('provider', 'openai')}")
    
    def _load_classification_rules(self) -> Dict[str, Any]:
        """åŠ è½½åˆ†ç±»è§„åˆ™é…ç½®"""
        try:
            rules_file = Path("config/classification_rules.yaml")
            if rules_file.exists():
                with open(rules_file, 'r', encoding='utf-8') as f:
                    rules = yaml.safe_load(f)
                self.logger.info("æˆåŠŸåŠ è½½åˆ†ç±»è§„åˆ™é…ç½®")
                return rules
            else:
                self.logger.warning("åˆ†ç±»è§„åˆ™é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™")
                return self._get_default_classification_rules()
        except Exception as e:
            self.logger.error(f"åŠ è½½åˆ†ç±»è§„åˆ™å¤±è´¥: {e}")
            return self._get_default_classification_rules()
    
    def _get_default_classification_rules(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤åˆ†ç±»è§„åˆ™"""
        return {
            'classification_rules': {
                'æŠ€æœ¯å¼€å‘': {
                    'keywords': ['æŠ€æœ¯', 'å¼€å‘', 'ç³»ç»Ÿ', 'è½¯ä»¶', 'tech', 'dev'],
                    'target_patterns': ['æŠ€æœ¯', 'å¼€å‘'],
                    'priority': 1
                },
                'é¡¹ç›®ç®¡ç†': {
                    'keywords': ['é¡¹ç›®', 'ç®¡ç†', 'project'],
                    'target_patterns': ['é¡¹ç›®', 'ç®¡ç†'],
                    'priority': 2
                },
                'ä¼šè®®æ²Ÿé€š': {
                    'keywords': ['ä¼šè®®', 'çºªè¦', 'meeting'],
                    'target_patterns': ['ä¼šè®®', 'æ²Ÿé€š'],
                    'priority': 3
                }
            },
            'fallback_folders': ['æ–‡æ¡£', 'å…¶ä»–', 'documents'],
            'strategy': {
                'semantic_threshold': 0.3,
                'allow_new_folders': False,
                'force_existing': True
            }
        }
    
    def extract_subject_and_folder(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        æå–æ–‡æ¡£ä¸»ä½“å¹¶å»ºè®®æ–‡ä»¶å¤¹è·¯å¾„
        
        Args:
            file_info: æ–‡ä»¶ä¿¡æ¯å­—å…¸
            
        Returns:
            åŒ…å«ä¸»ä½“å’Œå»ºè®®æ–‡ä»¶å¤¹è·¯å¾„çš„å­—å…¸
        """
        if not self.enabled:
            return {
                'subject': self._extract_fallback_subject(file_info),
                'suggested_folder': '',
                'confidence': 0.0,
                'reasoning': 'æœªé…ç½®LLM APIï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ'
            }
        
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_subject_extraction_prompt(file_info)
            
            # è°ƒç”¨LLM API
            response = self._call_llm_api(prompt)
            
            # è§£æå“åº”
            result = self._parse_subject_response(response)
            
            self.logger.info(f"LLMæå–ä¸»ä½“æˆåŠŸ: {result['subject']}")
            return result
            
        except Exception as e:
            self.logger.error(f"LLMæå–ä¸»ä½“å¤±è´¥: {e}")
            return {
                'subject': self._extract_fallback_subject(file_info),
                'suggested_folder': '',
                'confidence': 0.0,
                'reasoning': f'LLMè°ƒç”¨å¤±è´¥: {str(e)}'
            }
    
    def check_content_similarity(self, content1: str, content2: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥ä¸¤ä¸ªæ–‡æ¡£å†…å®¹çš„ç›¸ä¼¼æ€§
        
        Args:
            content1: ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹
            content2: ç¬¬äºŒä¸ªæ–‡æ¡£å†…å®¹
            
        Returns:
            ç›¸ä¼¼æ€§åˆ†æç»“æœ
        """
        if not self.enabled:
            return {
                'is_similar': False,
                'similarity_score': 0.0,
                'reasoning': 'æœªé…ç½®LLM API'
            }
        
        try:
            prompt = self._build_similarity_prompt(content1, content2)
            response = self._call_llm_api(prompt)
            result = self._parse_similarity_response(response)
            
            self.logger.info(f"å†…å®¹ç›¸ä¼¼æ€§æ£€æŸ¥å®Œæˆ: {result['similarity_score']}")
            return result
            
        except Exception as e:
            self.logger.error(f"ç›¸ä¼¼æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return {
                'is_similar': False,
                'similarity_score': 0.0,
                'reasoning': f'LLMè°ƒç”¨å¤±è´¥: {str(e)}'
            }
    
    def suggest_folder_structure(self, subject: str, existing_folders: List[str]) -> Dict[str, Any]:
        """
        åŸºäºä¸»ä½“å’Œç°æœ‰æ–‡ä»¶å¤¹å»ºè®®åˆé€‚çš„æ–‡ä»¶å¤¹ç»“æ„
        
        Args:
            subject: æ–‡æ¡£ä¸»ä½“
            existing_folders: ç°æœ‰æ–‡ä»¶å¤¹åˆ—è¡¨
            
        Returns:
            æ–‡ä»¶å¤¹å»ºè®®ç»“æœ
        """
        if not self.enabled:
            return {
                'suggested_path': subject,
                'create_new': True,
                'reasoning': 'æœªé…ç½®LLM APIï¼Œä½¿ç”¨ä¸»ä½“ä½œä¸ºæ–‡ä»¶å¤¹å'
            }
        
        try:
            self.logger.info(f"å¼€å§‹æ–‡ä»¶å¤¹å»ºè®® - ä¸»ä½“: {subject}")
            
            # é‡æ–°æ‰«ææ–‡ä»¶å¤¹ç»“æ„ï¼Œç¡®ä¿è·å–æœ€æ–°çš„æ‰‹åŠ¨ä¿®æ”¹
            refreshed_folders = self._scan_current_folders()
            combined_folders = list(set(existing_folders + refreshed_folders))
            self.logger.info(f"åˆå¹¶åçš„æ–‡ä»¶å¤¹åˆ—è¡¨: {combined_folders}")
            
            # ç¬¬ä¸€æ­¥ï¼šå°è¯•ç²¾ç¡®æ–‡æœ¬åŒ¹é…
            exact_match = self._find_exact_folder_match(subject, combined_folders)
            if exact_match:
                self.logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œè¿”å›: {exact_match}")
                return {
                    'suggested_path': exact_match,
                    'create_new': False,
                    'reasoning': f'æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„æ–‡ä»¶å¤¹: {exact_match}'
                }
            
            # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨é…ç½®åŒ–è¯­ä¹‰åˆ†æåŒ¹é…
            semantic_match = self._find_semantic_folder_match(subject, combined_folders)
            if semantic_match:
                self.logger.info(f"æ‰¾åˆ°è¯­ä¹‰åŒ¹é…ï¼Œè¿”å›: {semantic_match}")
                return {
                    'suggested_path': semantic_match,
                    'create_new': False,
                    'reasoning': f'é€šè¿‡è¯­ä¹‰åˆ†ææ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¤¹: {semantic_match}'
                }
            
            # ç¬¬äºŒæ­¥è¡¥å……ï¼šå¦‚æœè¯­ä¹‰åŒ¹é…æ‰¾åˆ°äº†ç±»åˆ«ä½†æ²¡æœ‰å¯¹åº”ç°æœ‰æ–‡ä»¶å¤¹ï¼Œåˆ›å»ºæ–°æ–‡ä»¶å¤¹
            semantic_category = self._find_semantic_category_match(subject)
            if semantic_category:
                self.logger.info(f"è¯­ä¹‰åˆ†ææ‰¾åˆ°ç±»åˆ«ä½†æ— ç°æœ‰æ–‡ä»¶å¤¹ï¼Œåˆ›å»ºæ–°æ–‡ä»¶å¤¹: {semantic_category}")
                return {
                    'suggested_path': semantic_category,
                    'create_new': True,
                    'reasoning': f'é€šè¿‡è¯­ä¹‰åˆ†æåˆ›å»ºæ–°ç±»åˆ«æ–‡ä»¶å¤¹: {semantic_category}'
                }
            
            # ç¬¬ä¸‰æ­¥ï¼šå°è¯•ç›¸ä¼¼æ€§åŒ¹é…
            similar_match = self._find_similar_folder_match(subject, combined_folders)
            if similar_match:
                self.logger.info(f"æ‰¾åˆ°ç›¸ä¼¼åŒ¹é…ï¼Œè¿”å›: {similar_match}")
                return {
                    'suggested_path': similar_match,
                    'create_new': False,
                    'reasoning': f'æ‰¾åˆ°ç›¸ä¼¼åŒ¹é…çš„æ–‡ä»¶å¤¹: {similar_match}'
                }
            
            self.logger.info("æœªæ‰¾åˆ°ç²¾ç¡®ã€è¯­ä¹‰æˆ–ç›¸ä¼¼åŒ¹é…ï¼Œè°ƒç”¨LLM")
            
            # ç¬¬å››æ­¥ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œè°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½å»ºè®®
            prompt = self._build_strict_folder_suggestion_prompt(subject, combined_folders)
            response = self._call_llm_api(prompt)
            result = self._parse_folder_response(response)
            
            self.logger.info(f"LLMåŸå§‹å“åº”ç»“æœ: {result}")
            
            # éªŒè¯LLMå»ºè®®çš„è·¯å¾„æ˜¯å¦ç¡®å®å­˜åœ¨
            if not result.get('create_new', True):
                suggested_path = result.get('suggested_path', '')
                # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦è¿›è¡Œæ¯”è¾ƒ
                normalized_suggested = suggested_path.replace('\\', '/')
                normalized_folders = [f.replace('\\', '/') for f in combined_folders]
                
                if normalized_suggested not in normalized_folders:
                    self.logger.warning(f"LLMå»ºè®®çš„è·¯å¾„ä¸å­˜åœ¨: {suggested_path}ï¼Œæ”¹ä¸ºåˆ›å»ºæ–°æ–‡ä»¶å¤¹")
                    result['create_new'] = True
                    result['reasoning'] = f"LLMå»ºè®®çš„è·¯å¾„ä¸å­˜åœ¨ï¼Œæ”¹ä¸ºåˆ›å»º: {suggested_path}"
                else:
                    self.logger.info(f"LLMå»ºè®®çš„è·¯å¾„éªŒè¯é€šè¿‡: {suggested_path}")
            
            # ç¬¬äº”æ­¥ï¼šå¦‚æœLLMè¯•å›¾åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼Œè¿›è¡Œæœ€ç»ˆå¤„ç†
            if result.get('create_new', True):
                # å¦‚æœæ²¡æœ‰ä»»ä½•ç°æœ‰æ–‡ä»¶å¤¹ï¼Œå…è®¸åˆ›å»ºè¯­ä¹‰åŒ¹é…çš„åˆ†ç±»æ–‡ä»¶å¤¹
                if not combined_folders:
                    semantic_category = self._find_semantic_category_match(subject)
                    if semantic_category:
                        self.logger.info(f"æ— ç°æœ‰æ–‡ä»¶å¤¹æ—¶ï¼Œå…è®¸åˆ›å»ºè¯­ä¹‰åˆ†ç±»æ–‡ä»¶å¤¹: {semantic_category}")
                        result = {
                            'suggested_path': semantic_category,
                            'create_new': True,
                            'reasoning': f'æ— ç°æœ‰æ–‡ä»¶å¤¹ï¼Œåˆ›å»ºè¯­ä¹‰åˆ†ç±»æ–‡ä»¶å¤¹: {semantic_category}'
                        }
                    else:
                        self.logger.info(f"æ— è¯­ä¹‰åŒ¹é…ï¼Œä½¿ç”¨ä¸»ä½“ä½œä¸ºæ–‡ä»¶å¤¹å: {subject}")
                        result = {
                            'suggested_path': subject,
                            'create_new': True,
                            'reasoning': f'æ— ç°æœ‰æ–‡ä»¶å¤¹å’Œè¯­ä¹‰åŒ¹é…ï¼Œä½¿ç”¨ä¸»ä½“åç§°: {subject}'
                        }
                else:
                    # æœ‰ç°æœ‰æ–‡ä»¶å¤¹ä½†LLMè¯•å›¾åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼Œå¼ºåˆ¶é€‰æ‹©ç°æœ‰åˆ†ç±»
                    self.logger.warning("LLMè¯•å›¾åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼Œå¯åŠ¨å¼ºåˆ¶åŒ¹é…")
                    forced_match = self._force_existing_folder_match(subject, combined_folders)
                    if forced_match:
                        result = {
                            'suggested_path': forced_match,
                            'create_new': False,
                            'reasoning': f'å¼ºåˆ¶ä½¿ç”¨ç°æœ‰æ–‡ä»¶å¤¹: {forced_match}ï¼ˆLLMè¯•å›¾åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼‰'
                        }
                        self.logger.warning(f"LLMè¯•å›¾åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼Œå¼ºåˆ¶ä½¿ç”¨ç°æœ‰åˆ†ç±»: {forced_match}")
                    else:
                        # å¦‚æœå¼ºåˆ¶åŒ¹é…ä¹Ÿå¤±è´¥ï¼Œå…è®¸åˆ›å»ºè¯­ä¹‰åˆ†ç±»æ–‡ä»¶å¤¹
                        semantic_category = self._find_semantic_category_match(subject)
                        if semantic_category:
                            self.logger.info(f"å¼ºåˆ¶åŒ¹é…å¤±è´¥ï¼Œåˆ›å»ºè¯­ä¹‰åˆ†ç±»æ–‡ä»¶å¤¹: {semantic_category}")
                            result = {
                                'suggested_path': semantic_category,
                                'create_new': True,
                                'reasoning': f'å¼ºåˆ¶åŒ¹é…å¤±è´¥ï¼Œåˆ›å»ºè¯­ä¹‰åˆ†ç±»æ–‡ä»¶å¤¹: {semantic_category}'
                            }
            
            self.logger.info(f"æ–‡ä»¶å¤¹å»ºè®®å®Œæˆ: {result['suggested_path']}")
            return result
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶å¤¹å»ºè®®å¤±è´¥: {e}")
            return {
                'suggested_path': subject,
                'create_new': True,
                'reasoning': f'LLMè°ƒç”¨å¤±è´¥: {str(e)}'
            }
    
    def _scan_current_folders(self) -> List[str]:
        """å®æ—¶æ‰«æå½“å‰çŸ¥è¯†åº“æ–‡ä»¶å¤¹ç»“æ„"""
        folders = []
        try:
            kb_path = Path("knowledge_base")
            if kb_path.exists():
                for root, dirs, files in os.walk(kb_path):
                    for dir_name in dirs:
                        full_path = Path(root) / dir_name
                        relative_path = full_path.relative_to(kb_path)
                        folders.append(str(relative_path))
        except Exception as e:
            self.logger.warning(f"æ‰«ææ–‡ä»¶å¤¹å¤±è´¥: {e}")
        return folders
    
    def _serialize_metadata_safely(self, metadata: Dict[str, Any]) -> str:
        """å®‰å…¨åœ°åºåˆ—åŒ–metadataï¼Œå¤„ç†datetimeå¯¹è±¡"""
        if not metadata:
            return 'æ— '
        
        safe_metadata = {}
        for key, value in metadata.items():
            if isinstance(value, datetime):
                safe_metadata[key] = value.strftime('%Y-%m-%d %H:%M:%S')
            elif hasattr(value, '__str__'):
                safe_metadata[key] = str(value)
            else:
                safe_metadata[key] = value
                
        try:
            return json.dumps(safe_metadata, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.warning(f"åºåˆ—åŒ–metadataå¤±è´¥: {e}")
            return str(safe_metadata)
    
    def _build_subject_extraction_prompt(self, file_info: Dict[str, Any]) -> str:
        """æ„å»ºä¸»ä½“æå–æç¤ºè¯"""
        content = file_info.get('content', '')[:3000]  # é™åˆ¶å†…å®¹é•¿åº¦
        metadata = file_info.get('metadata', {})
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œæå–æ ¸å¿ƒä¸»ä½“å¹¶å»ºè®®åˆé€‚çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚

æ–‡ä»¶å: {file_info['name']}
æ–‡ä»¶ç±»å‹: {file_info['suffix']}

å…ƒæ•°æ®ä¿¡æ¯:
{self._serialize_metadata_safely(metadata)}

æ–‡æ¡£å†…å®¹:
{content}

**é‡è¦è¯´æ˜ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ–‡å­—è¯´æ˜**

{{
    "subject": "æ–‡æ¡£çš„æ ¸å¿ƒä¸»ä½“ï¼Œç”¨ä½œé‡å‘½åçš„åŸºç¡€ï¼ˆç®€æ´æ˜äº†ï¼Œé€‚åˆä½œä¸ºæ–‡ä»¶åï¼‰",
    "suggested_folder": "å»ºè®®çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¦‚æœéœ€è¦å¤šå±‚çº§ï¼Œç”¨/åˆ†éš”ï¼‰",
    "confidence": 0.85,
    "reasoning": "æå–ä¸»ä½“å’Œå»ºè®®æ–‡ä»¶å¤¹çš„ç†ç”±"
}}

æ–‡ä»¶å¤¹å»ºè®®åŸåˆ™ï¼š
1. ä¼˜å…ˆæŒ‰ä¸»é¢˜åˆ†ç±»ï¼šä¼šè®®çºªè¦ã€é¡¹ç›®æ–‡æ¡£ã€æŠ€æœ¯æ–¹æ¡ˆã€è´¢åŠ¡æŠ¥å‘Šã€äººåŠ›èµ„æºç­‰
2. é¿å…ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºå•ç‹¬æ–‡ä»¶å¤¹ï¼Œåº”å½’ç±»åˆ°åˆé€‚çš„ä¸»é¢˜æ–‡ä»¶å¤¹
3. æ–‡ä»¶å¤¹åç§°ç®€æ´æ¸…æ™°ï¼Œé¿å…è¿‡é•¿
4. è€ƒè™‘æ–‡æ¡£çš„ä¸šåŠ¡ç±»å‹å’Œç”¨é€”
"""
        return prompt
    
    def _build_similarity_prompt(self, content1: str, content2: str) -> str:
        """æ„å»ºç›¸ä¼¼æ€§æ£€æŸ¥æç¤ºè¯"""
        # é™åˆ¶å†…å®¹é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶
        content1 = content1[:2000]
        content2 = content2[:2000]
        
        prompt = f"""
è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ–‡æ¡£å†…å®¹çš„ç›¸ä¼¼æ€§ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦å¯èƒ½æ˜¯åŒä¸€ä¸»é¢˜çš„ä¸åŒç‰ˆæœ¬ã€‚

æ–‡æ¡£1å†…å®¹:
{content1}

æ–‡æ¡£2å†…å®¹:
{content2}

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "is_similar": true/false,
    "similarity_score": ç›¸ä¼¼åº¦åˆ†æ•°(0-1),
    "reasoning": "ç›¸ä¼¼æ€§åˆ¤æ–­çš„ç†ç”±"
}}

åˆ¤æ–­æ ‡å‡†ï¼š
- å¦‚æœä¸¤ä¸ªæ–‡æ¡£è®¨è®ºçš„æ˜¯åŒä¸€ä¸ªä¸»é¢˜ã€é¡¹ç›®æˆ–äº‹ä»¶ï¼Œå³ä½¿å†…å®¹æœ‰å·®å¼‚ä¹Ÿåº”è¯¥è¢«è®¤ä¸ºæ˜¯ç›¸ä¼¼çš„
- ç›¸ä¼¼åº¦åˆ†æ•°åº”è¯¥åæ˜ ä¸»é¢˜ç›¸å…³æ€§è€Œä¸æ˜¯æ–‡å­—é‡å¤åº¦
- 0.7ä»¥ä¸Šå¯ä»¥è®¤ä¸ºæ˜¯åŒä¸€ä¸»é¢˜çš„ä¸åŒç‰ˆæœ¬
"""
        return prompt
    
    def _build_strict_folder_suggestion_prompt(self, subject: str, existing_folders: List[str]) -> str:
        """æ„å»ºä¸¥æ ¼çš„æ–‡ä»¶å¤¹å»ºè®®æç¤ºè¯"""
        existing_str = "\n".join(f"- {folder}" for folder in existing_folders) if existing_folders else "æš‚æ— ç°æœ‰æ–‡ä»¶å¤¹"
        
        # è¯»å–çŸ¥è¯†åº“ç»“æ„æ–‡ä»¶
        structure_content = self._read_knowledge_base_structure()
        
        prompt = f"""
è¯·ä¸ºæ–‡æ¡£ä¸»ä½“"{subject}"å»ºè®®åˆé€‚çš„æ–‡ä»¶å¤¹å½’æ¡£è·¯å¾„ã€‚

**å½“å‰çŸ¥è¯†åº“æ–‡ä»¶å¤¹ç»“æ„å‚è€ƒï¼š**
{structure_content}

**ç°æœ‰æ–‡ä»¶å¤¹å®Œæ•´åˆ—è¡¨ï¼š**
{existing_str}

**ç»å¯¹ä¸¥æ ¼è¦æ±‚ï¼ˆè¿åå°†å¯¼è‡´å¤„ç†å¤±è´¥ï¼‰ï¼š**
1. **suggested_pathå¿…é¡»å®Œå…¨ç­‰äºç°æœ‰æ–‡ä»¶å¤¹åˆ—è¡¨ä¸­çš„æŸä¸€é¡¹**
2. **ç»å¯¹ç¦æ­¢åˆ›å»ºä»»ä½•æ–°æ–‡ä»¶å¤¹ï¼Œcreate_newå¿…é¡»ä¸ºfalse**
3. **å¿…é¡»ä»ä¸Šè¿°ç°æœ‰æ–‡ä»¶å¤¹åˆ—è¡¨ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ª**
4. **å¦‚æœä¸»ä½“åŒ…å«"è¿ç»´"ã€"ç›‘æ§"ã€"éƒ¨ç½²"å…³é”®è¯ï¼Œå¿…é¡»é€‰æ‹©"æŠ€æœ¯æ–¹æ¡ˆ/DevOpsè¿ç»´"**
5. **å¦‚æœä¸»ä½“åŒ…å«"å®¹å™¨"ã€"Docker"å…³é”®è¯ï¼Œå¿…é¡»é€‰æ‹©"æŠ€æœ¯æ–¹æ¡ˆ/å®¹å™¨åŒ–éƒ¨ç½²"**

**å¼ºåˆ¶é€‰æ‹©é€»è¾‘ï¼ˆæŒ‰æ­¤ä¼˜å…ˆçº§æ‰§è¡Œï¼‰ï¼š**
- è¿ç»´/ç›‘æ§/éƒ¨ç½²ç›¸å…³ â†’ å¼ºåˆ¶é€‰æ‹© "æŠ€æœ¯æ–¹æ¡ˆ/DevOpsè¿ç»´"
- å®¹å™¨/Dockerç›¸å…³ â†’ å¼ºåˆ¶é€‰æ‹© "æŠ€æœ¯æ–¹æ¡ˆ/å®¹å™¨åŒ–éƒ¨ç½²"  
- é¡¹ç›®ç®¡ç†ç›¸å…³ â†’ å¼ºåˆ¶é€‰æ‹© "é¡¹ç›®æ–‡æ¡£/é¡¹ç›®ç®¡ç†"
- ä¼šè®®è®°å½•ç›¸å…³ â†’ å¼ºåˆ¶é€‰æ‹© "ä¼šè®®çºªè¦"
- åŸ¹è®­ç›¸å…³ â†’ å¼ºåˆ¶é€‰æ‹© "äººåŠ›èµ„æº/åŸ¹è®­è®¡åˆ’"
- é¢è¯•ç›¸å…³ â†’ å¼ºåˆ¶é€‰æ‹© "äººåŠ›èµ„æº/é¢è¯•è®°å½•"
- å…¶ä»–æŠ€æœ¯æ–‡æ¡£ â†’ å¼ºåˆ¶é€‰æ‹© "æŠ€æœ¯æ–¹æ¡ˆ"

**è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼š**

{{
    "suggested_path": "å¿…é¡»æ˜¯ç°æœ‰æ–‡ä»¶å¤¹åˆ—è¡¨ä¸­çš„å®Œæ•´è·¯å¾„",
    "create_new": false,
    "reasoning": "è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªç°æœ‰æ–‡ä»¶å¤¹"
}}

**æœ€åæ£€æŸ¥ï¼šè¿”å›å‰ç¡®è®¤suggested_pathç¡®å®åœ¨ç°æœ‰æ–‡ä»¶å¤¹åˆ—è¡¨ä¸­ï¼**
"""
        return prompt
    
    def _read_knowledge_base_structure(self) -> str:
        """è¯»å–çŸ¥è¯†åº“ç»“æ„æ–‡ä»¶"""
        try:
            structure_file = Path("knowledge_base/structure.md")
            if structure_file.exists():
                with open(structure_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                # æå–æ–‡ä»¶å¤¹ç»“æ„éƒ¨åˆ†ï¼ˆä»"å½“å‰æ–‡ä»¶å¤¹ç»“æ„"åˆ°"åˆ†ç±»è§„åˆ™"ï¼‰
                start_marker = "## ğŸ—‚ï¸ å½“å‰æ–‡ä»¶å¤¹ç»“æ„"
                end_marker = "## ğŸ“‹ åˆ†ç±»è§„åˆ™"
                
                start_idx = content.find(start_marker)
                end_idx = content.find(end_marker)
                
                if start_idx != -1 and end_idx != -1:
                    structure_section = content[start_idx:end_idx].strip()
                    return structure_section
                else:
                    return content[:1000]  # è¿”å›å‰1000å­—ç¬¦ä½œä¸ºå¤‡é€‰
            else:
                return "çŸ¥è¯†åº“ç»“æ„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æŒ‰æ ‡å‡†åˆ†ç±»åˆ›å»ºæ–‡ä»¶å¤¹"
        except Exception as e:
            self.logger.warning(f"è¯»å–çŸ¥è¯†åº“ç»“æ„å¤±è´¥: {e}")
            return "æ— æ³•è¯»å–çŸ¥è¯†åº“ç»“æ„ï¼Œè¯·æŒ‰æ ‡å‡†åˆ†ç±»åˆ›å»ºæ–‡ä»¶å¤¹"
    
    def _call_llm_api(self, prompt: str) -> str:
        """è°ƒç”¨LLM API"""
        provider = self.config.get('provider', 'openai').lower()
        
        if provider == 'openai':
            return self._call_openai_api(prompt)
        elif provider == 'anthropic':
            return self._call_anthropic_api(prompt)
        elif provider == 'zhipu':
            return self._call_zhipu_api(prompt)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
    
    def _call_openai_api(self, prompt: str) -> str:
        """è°ƒç”¨OpenAI API"""
        url = self.config.get('base_url', 'https://api.openai.com/v1') + '/chat/completions'
        
        headers = {
            'Authorization': f"Bearer {self.config['api_key']}",
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': self.config.get('model', 'gpt-3.5-turbo'),
            'messages': [
                {'role': 'user', 'content': prompt}
            ],
            'temperature': 0.3,
            'max_tokens': 1000
        }
        
        response = requests.post(
            url, 
            headers=headers, 
            json=data, 
            timeout=self.config.get('timeout', 30)
        )
        
        if response.status_code != 200:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
        
        result = response.json()
        return result['choices'][0]['message']['content']
    
    def _call_anthropic_api(self, prompt: str) -> str:
        """è°ƒç”¨Anthropic APIï¼ˆClaudeï¼‰"""
        url = self.config.get('base_url', 'https://api.anthropic.com/v1') + '/messages'
        
        headers = {
            'x-api-key': self.config['api_key'],
            'Content-Type': 'application/json',
            'anthropic-version': '2023-06-01'
        }
        
        data = {
            'model': self.config.get('model', 'claude-3-haiku-20240307'),
            'max_tokens': 1000,
            'messages': [
                {'role': 'user', 'content': prompt}
            ]
        }
        
        response = requests.post(
            url, 
            headers=headers, 
            json=data, 
            timeout=self.config.get('timeout', 30)
        )
        
        if response.status_code != 200:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
        
        result = response.json()
        return result['content'][0]['text']
    
    def _call_zhipu_api(self, prompt: str) -> str:
        """è°ƒç”¨æ™ºè°±AI API"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®æ™ºè°±AIçš„å®é™…APIå®ç°
        raise NotImplementedError("æ™ºè°±AIæ¥å£å°šæœªå®ç°")
    
    def _parse_subject_response(self, response: str) -> Dict[str, Any]:
        """è§£æä¸»ä½“æå–å“åº”"""
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æJSON
            result = json.loads(response.strip())
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            if 'subject' not in result:
                raise ValueError("å“åº”ä¸­ç¼ºå°‘subjectå­—æ®µ")
            
            # æ¸…ç†ä¸»ä½“åç§°ï¼Œå»é™¤ä¸ç¬¦åˆæ–‡ä»¶ç³»ç»Ÿè§„èŒƒçš„å­—ç¬¦
            subject = result.get('subject', '').strip()
            subject = self._clean_filename(subject)
            
            return {
                'subject': subject,
                'suggested_folder': result.get('suggested_folder', '').strip(),
                'confidence': float(result.get('confidence', 0.5)),
                'reasoning': result.get('reasoning', '')
            }
            
        except (json.JSONDecodeError, ValueError) as e:
            self.logger.warning(f"å°è¯•JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•å…¶ä»–è§£ææ–¹æ³•")
            
            # å°è¯•æå–è¢«```åŒ…å›´çš„JSON
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group(1))
                    subject = self._clean_filename(result.get('subject', ''))
                    if subject:
                        return {
                            'subject': subject,
                            'suggested_folder': result.get('suggested_folder', '').strip(),
                            'confidence': float(result.get('confidence', 0.5)),
                            'reasoning': result.get('reasoning', '')
                        }
                except (json.JSONDecodeError, ValueError):
                    pass
            
            # å°è¯•æŸ¥æ‰¾å•ç‹¬JSONå¯¹è±¡
            json_match = re.search(r'\{[^{}]*"subject"[^{}]*\}', response, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group(0))
                    subject = self._clean_filename(result.get('subject', ''))
                    if subject:
                        return {
                            'subject': subject,
                            'suggested_folder': result.get('suggested_folder', '').strip(),
                            'confidence': float(result.get('confidence', 0.5)),
                            'reasoning': result.get('reasoning', '')
                        }
                except (json.JSONDecodeError, ValueError):
                    pass
            
            self.logger.warning(f"æ‰€æœ‰è§£ææ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æå–")
            
            # ç®€å•çš„æ–‡æœ¬æå–ä½œä¸ºæœ€åå›é€€
            lines = response.strip().split('\n')
            subject = ''
            suggested_folder = ''
            
            for line in lines:
                line = line.strip()
                if ('subject' in line.lower() or 'ä¸»ä½“' in line) and ':' in line:
                    subject = line.split(':')[-1].strip().strip('"\'""''')
                elif ('folder' in line.lower() or 'æ–‡ä»¶å¤¹' in line) and ':' in line:
                    suggested_folder = line.split(':')[-1].strip().strip('"\'""''')
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥æå–æ–‡æ¡£å†…å®¹ä¸­çš„æ ‡é¢˜
            if not subject:
                # å°è¯•ä»åŸå§‹å“åº”ä¸­æå–å¯èƒ½çš„ä¸»ä½“ä¿¡æ¯
                potential_subjects = re.findall(r'["\']([^"\']{3,50})["\']', response)
                for ps in potential_subjects:
                    if ps and not any(skip in ps.lower() for skip in ['json', 'format', 'æ ¼å¼', 'ç¤ºä¾‹']):
                        subject = ps
                        break
            
            # æ¸…ç†æå–çš„ä¸»ä½“åç§°
            subject = self._clean_filename(subject) if subject else 'æœªçŸ¥ä¸»ä½“'
            
            return {
                'subject': subject,
                'suggested_folder': self._clean_filename(suggested_folder),
                'confidence': 0.3,
                'reasoning': 'å“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æå–'
            }
    
    def _clean_filename(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œå»é™¤ä¸ç¬¦åˆæ–‡ä»¶ç³»ç»Ÿè§„èŒƒçš„å­—ç¬¦"""
        if not name:
            return name
        
        # å»é™¤é¦–å°¾çš„å¼•å·å’Œç©ºæ ¼
        name = name.strip().strip('"\'""''')
        
        # æ›¿æ¢ä¸ç¬¦åˆWindowsæ–‡ä»¶ç³»ç»Ÿè§„èŒƒçš„å­—ç¬¦
        invalid_chars = ['<', '>', ':', '"', '|', '?', '*', '/', '\\']
        for char in invalid_chars:
            name = name.replace(char, '_')
        
        # å»é™¤å°¾éƒ¨çš„ç‚¹ã€ç©ºæ ¼ã€é€—å·ã€ä¸‹åˆ’çº¿ç­‰
        name = name.rstrip('.,_ ')
        
        # é™åˆ¶é•¿åº¦ï¼ˆWindowsè·¯å¾„é™åˆ¶ï¼‰
        if len(name) > 100:
            name = name[:100].rstrip('.,_ ')
        
        return name or 'æœªåˆ†ç±»æ–‡æ¡£'
    
    def _parse_similarity_response(self, response: str) -> Dict[str, Any]:
        """è§£æç›¸ä¼¼æ€§æ£€æŸ¥å“åº”"""
        try:
            result = json.loads(response)
            return {
                'is_similar': bool(result.get('is_similar', False)),
                'similarity_score': float(result.get('similarity_score', 0.0)),
                'reasoning': result.get('reasoning', '')
            }
        except (json.JSONDecodeError, ValueError):
            return {
                'is_similar': False,
                'similarity_score': 0.0,
                'reasoning': 'å“åº”è§£æå¤±è´¥'
            }
    
    def _parse_folder_response(self, response: str) -> Dict[str, Any]:
        """è§£ææ–‡ä»¶å¤¹å»ºè®®å“åº”"""
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æJSON
            result = json.loads(response.strip())
            return {
                'suggested_path': result.get('suggested_path', '').strip(),
                'create_new': bool(result.get('create_new', True)),
                'reasoning': result.get('reasoning', '')
            }
        except (json.JSONDecodeError, ValueError) as e:
            self.logger.warning(f"å°è¯•JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•å…¶ä»–è§£ææ–¹æ³•")
            
            # å°è¯•æå–è¢«```åŒ…å›´çš„JSON
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group(1))
                    return {
                        'suggested_path': result.get('suggested_path', '').strip(),
                        'create_new': bool(result.get('create_new', True)),
                        'reasoning': result.get('reasoning', '')
                    }
                except (json.JSONDecodeError, ValueError):
                    pass
            
            # å°è¯•æŸ¥æ‰¾å•ç‹¬JSONå¯¹è±¡
            json_match = re.search(r'\{[^{}]*"suggested_path"[^{}]*\}', response, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group(0))
                    return {
                        'suggested_path': result.get('suggested_path', '').strip(),
                        'create_new': bool(result.get('create_new', True)),
                        'reasoning': result.get('reasoning', '')
                    }
                except (json.JSONDecodeError, ValueError):
                    pass
            
            # æ–‡æœ¬æå–ä½œä¸ºå›é€€
            lines = response.strip().split('\n')
            suggested_path = ''
            reasoning = ''
            
            for line in lines:
                line = line.strip()
                if ('path' in line.lower() or 'è·¯å¾„' in line or 'æ–‡ä»¶å¤¹' in line) and ':' in line:
                    suggested_path = line.split(':')[-1].strip().strip('"\'""''')
                elif ('reason' in line.lower() or 'ç†ç”±' in line) and ':' in line:
                    reasoning = line.split(':')[-1].strip().strip('"\'""''')
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»å“åº”ä¸­æå–å¯èƒ½çš„è·¯å¾„
            if not suggested_path:
                # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶å¤¹è·¯å¾„
                folder_patterns = [
                    r'ä¼šè®®çºªè¦', r'é¡¹ç›®æ–‡æ¡£', r'æŠ€æœ¯æ–‡æ¡£', r'è´¢åŠ¡æŠ¥å‘Š', r'äººåŠ›èµ„æº',
                    r'ç®¡ç†åˆ¶åº¦', r'å®¢æˆ·èµ„æ–™', r'åŸ¹è®­ææ–™', r'æŠ€æœ¯æ–¹æ¡ˆ', r'é¡¹ç›®ç®¡ç†'
                ]
                for pattern in folder_patterns:
                    if re.search(pattern, response, re.IGNORECASE):
                        suggested_path = pattern
                        break
            
            return {
                'suggested_path': self._clean_filename(suggested_path),
                'create_new': True,
                'reasoning': reasoning or 'å“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æå–'
            }
    
    def _extract_fallback_subject(self, file_info: Dict[str, Any]) -> str:
        """æå–ä¸»ä½“çš„å›é€€æ–¹æ¡ˆï¼ˆä¸ä¾èµ–LLMï¼‰"""
        # ä¼˜å…ˆä½¿ç”¨æ–‡æ¡£å…ƒæ•°æ®ä¸­çš„æ ‡é¢˜
        metadata = file_info.get('metadata', {})
        if metadata.get('title'):
            return metadata['title']
        
        # ä½¿ç”¨æ–‡ä»¶åï¼ˆå»æ‰æ‰©å±•åï¼‰
        stem = file_info.get('stem', '')
        if stem:
            return stem
        
        # æœ€åçš„å›é€€
        return 'æœªåˆ†ç±»æ–‡æ¡£'
    
    def _find_exact_folder_match(self, subject: str, folders: List[str]) -> Optional[str]:
        """æŸ¥æ‰¾ç²¾ç¡®åŒ¹é…çš„æ–‡ä»¶å¤¹ - åŸºäºæ–‡æœ¬ç›´æ¥åŒ¹é…"""
        subject_lower = subject.lower()
        self.logger.info(f"ç²¾ç¡®åŒ¹é…æ£€æŸ¥ - ä¸»ä½“: {subject_lower}")
        self.logger.info(f"å¯ç”¨æ–‡ä»¶å¤¹: {folders}")
        
        # ç›´æ¥æ–‡æœ¬åŒ…å«åŒ¹é…
        for folder_path in folders:
            folder_name = folder_path.split('/')[-1].lower()
            
            # æ£€æŸ¥æ–‡ä»¶å¤¹åæ˜¯å¦åŒ…å«åœ¨ä¸»ä½“ä¸­ï¼Œæˆ–ä¸»ä½“åŒ…å«æ–‡ä»¶å¤¹å
            if folder_name in subject_lower or any(word in folder_name for word in subject_lower.split()):
                self.logger.info(f"ç›´æ¥æ–‡æœ¬åŒ¹é…: {folder_path}")
                return folder_path
        
        self.logger.info("æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…")
        return None
    
    def _find_semantic_folder_match(self, subject: str, folders: List[str]) -> Optional[str]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§çš„æ–‡ä»¶å¤¹åŒ¹é… - ä½¿ç”¨é…ç½®åŒ–è§„åˆ™"""
        # è·å–åˆ†ç±»è§„åˆ™å’Œç­–ç•¥é…ç½®
        rules = self.classification_rules.get('classification_rules', {})
        strategy = self.classification_rules.get('strategy', {})
        threshold = strategy.get('semantic_threshold', 0.3)
        
        # è®¡ç®—æ¯ä¸ªè¯­ä¹‰ç±»åˆ«çš„åŒ¹é…åˆ†æ•°
        category_scores = {}
        
        for category, info in rules.items():
            keywords = info.get('keywords', [])
            priority = info.get('priority', 10)
            
            # å…³é”®è¯åŒ¹é…è®¡åˆ†
            matched_keywords = 0
            for keyword in keywords:
                if keyword.lower() in subject.lower():
                    matched_keywords += 1
            
            # è®¡ç®—å½’ä¸€åŒ–åˆ†æ•°ï¼ˆä¸è€ƒè™‘ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆçº§ä»…ç”¨äºæœ€ç»ˆé€‰æ‹©ï¼‰
            if matched_keywords > 0:
                normalized_score = matched_keywords / len(keywords)
                category_scores[category] = {
                    'score': normalized_score,
                    'priority': priority,
                    'matched_keywords': matched_keywords
                }
        
        # è¿‡æ»¤ä½äºé˜ˆå€¼çš„åˆ†æ•°
        valid_categories = {k: v for k, v in category_scores.items() if v['score'] >= threshold}
        
        if not valid_categories:
            self.logger.info(f"æ²¡æœ‰ç±»åˆ«è¾¾åˆ°é˜ˆå€¼ {threshold}")
            return None
        
        # åœ¨æœ‰æ•ˆç±»åˆ«ä¸­ï¼Œä¼˜å…ˆé€‰æ‹©åˆ†æ•°æœ€é«˜çš„ï¼Œåˆ†æ•°ç›¸åŒæ—¶é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„ï¼ˆæ•°å­—æœ€å°ï¼‰
        best_category = max(valid_categories.items(), 
                          key=lambda x: (x[1]['score'], -x[1]['priority']))
        
        best_category_name = best_category[0]
        best_info = best_category[1]
        best_patterns = rules[best_category_name].get('target_patterns', [])
        
        self.logger.info(f"æœ€ä½³è¯­ä¹‰ç±»åˆ«: {best_category_name}, åˆ†æ•°: {best_info['score']:.3f}, ä¼˜å…ˆçº§: {best_info['priority']}")
        
        # åœ¨ç°æœ‰æ–‡ä»¶å¤¹ä¸­å¯»æ‰¾åŒ¹é…è¯¥è¯­ä¹‰ç±»åˆ«çš„æ–‡ä»¶å¤¹
        for pattern in best_patterns:
            for folder_path in folders:
                if pattern in folder_path:
                    self.logger.info(f"è¯­ä¹‰åŒ¹é…æˆåŠŸ - ç±»åˆ«: {best_category_name}, æ¨¡å¼: {pattern}, æ–‡ä»¶å¤¹: {folder_path}")
                    return folder_path
        
        return None
    
    def _find_similar_folder_match(self, subject: str, folders: List[str]) -> Optional[str]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„æ–‡ä»¶å¤¹ï¼ˆåŸºäºç¼–è¾‘è·ç¦»æˆ–åŒ…å«å…³ç³»ï¼‰"""
        subject_lower = subject.lower()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶å¤¹ååŒ…å«åœ¨ä¸»ä½“ä¸­ï¼Œæˆ–ä¸»ä½“åŒ…å«æ–‡ä»¶å¤¹å
        best_match = None
        best_score = 0
        
        for folder_path in folders:
            folder_name = folder_path.split('/')[-1].lower()  # è·å–æ–‡ä»¶å¤¹åç§°
            
            # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
            score = 0
            
            # åŒ…å«å…³ç³»æ£€æŸ¥
            if folder_name in subject_lower or subject_lower in folder_name:
                score += 0.8
            
            # å…³é”®è¯é‡å æ£€æŸ¥
            subject_words = set(subject_lower.split())
            folder_words = set(folder_name.split())
            if subject_words & folder_words:  # æœ‰äº¤é›†
                score += 0.6
            
            # å¦‚æœåˆ†æ•°è¶³å¤Ÿé«˜ï¼Œè®°å½•ä¸ºæœ€ä½³åŒ¹é…
            if score > best_score and score >= 0.6:
                best_score = score
                best_match = folder_path
        
        return best_match 
    
    def _force_existing_folder_match(self, subject: str, folders: List[str]) -> Optional[str]:
        """å¼ºåˆ¶ä»ç°æœ‰æ–‡ä»¶å¤¹ä¸­é€‰æ‹©ä¸€ä¸ªåŒ¹é…çš„ - é€šç”¨åˆ†ç±»ç­–ç•¥"""
        subject_lower = subject.lower()
        
        # ç¬¬ä¸€æ­¥ï¼šå°è¯•è¯­ä¹‰åŒ¹é…
        semantic_match = self._find_semantic_folder_match(subject_lower, folders)
        if semantic_match:
            return semantic_match
        
        # ç¬¬äºŒæ­¥ï¼šåŸºäºé€šç”¨åˆ†ç±»ç­–ç•¥çš„å¼ºåˆ¶åˆ†é…
        # ä¼˜å…ˆçº§ä»é«˜åˆ°ä½çš„é€šç”¨åˆ†ç±»
        generic_classification_rules = [
            # æŠ€æœ¯ç›¸å…³
            {
                'keywords': ['æŠ€æœ¯', 'å¼€å‘', 'ç³»ç»Ÿ', 'è½¯ä»¶', 'ä»£ç ', 'ç¨‹åº', 'å·¥ç¨‹', 'tech', 'dev'],
                'target_patterns': ['æŠ€æœ¯', 'å¼€å‘', 'å·¥ç¨‹', 'tech']
            },
            # ç®¡ç†ç›¸å…³
            {
                'keywords': ['ç®¡ç†', 'é¡¹ç›®', 'è®¡åˆ’', 'è§„åˆ’', 'ç­–ç•¥', 'management', 'project'],
                'target_patterns': ['é¡¹ç›®', 'ç®¡ç†', 'project']
            },
            # ä¸šåŠ¡ç›¸å…³
            {
                'keywords': ['ä¸šåŠ¡', 'æµç¨‹', 'è§„èŒƒ', 'æ ‡å‡†', 'éœ€æ±‚', 'business'],
                'target_patterns': ['ä¸šåŠ¡', 'æµç¨‹', 'business']
            },
            # äººäº‹ç›¸å…³
            {
                'keywords': ['äººäº‹', 'äººåŠ›', 'å‘˜å·¥', 'æ‹›è˜', 'åŸ¹è®­', 'é¢è¯•', 'hr'],
                'target_patterns': ['äººåŠ›èµ„æº', 'äººäº‹', 'hr', 'åŸ¹è®­']
            },
            # è´¢åŠ¡ç›¸å…³
            {
                'keywords': ['è´¢åŠ¡', 'é¢„ç®—', 'æˆæœ¬', 'è´¹ç”¨', 'æŠ¥å‘Š', 'finance'],
                'target_patterns': ['è´¢åŠ¡', 'finance', 'æŠ¥å‘Š']
            },
            # ä¼šè®®äº¤æµç›¸å…³
            {
                'keywords': ['ä¼šè®®', 'çºªè¦', 'è®¨è®º', 'æ²Ÿé€š', 'meeting'],
                'target_patterns': ['ä¼šè®®', 'meeting', 'æ²Ÿé€š']
            },
            # å­¦ä¹ æˆé•¿ç›¸å…³
            {
                'keywords': ['å­¦ä¹ ', 'æˆé•¿', 'çŸ¥è¯†', 'æ•™è‚²', 'å“²å­¦', 'æ€è€ƒ'],
                'target_patterns': ['å­¦ä¹ ', 'æˆé•¿', 'çŸ¥è¯†', 'æ•™è‚²']
            }
        ]
        
        # æŒ‰è§„åˆ™æ£€æŸ¥å¹¶åˆ†é…
        for rule in generic_classification_rules:
            # æ£€æŸ¥ä¸»ä½“æ˜¯å¦åŒ…å«è¯¥åˆ†ç±»çš„å…³é”®è¯
            if any(keyword in subject_lower for keyword in rule['keywords']):
                # åœ¨ç°æœ‰æ–‡ä»¶å¤¹ä¸­å¯»æ‰¾åŒ¹é…çš„æ¨¡å¼
                for pattern in rule['target_patterns']:
                    for folder_path in folders:
                        if pattern in folder_path:
                            self.logger.info(f"é€šç”¨åˆ†ç±»åŒ¹é… - å…³é”®è¯: {rule['keywords']}, æ¨¡å¼: {pattern}, æ–‡ä»¶å¤¹: {folder_path}")
                            return folder_path
        
        # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨æœ€é€šç”¨çš„ç­–ç•¥
        return self._get_most_generic_folder(folders)
    
    def _get_most_generic_folder(self, folders: List[str]) -> Optional[str]:
        """è·å–æœ€é€šç”¨çš„æ–‡ä»¶å¤¹ä½œä¸ºå›é€€æ–¹æ¡ˆ - ä½¿ç”¨é…ç½®åŒ–å›é€€åˆ—è¡¨"""
        # ä»é…ç½®ä¸­è·å–å›é€€æ–‡ä»¶å¤¹åˆ—è¡¨
        fallback_list = self.classification_rules.get('fallback_folders', [
            'æ–‡æ¡£', 'èµ„æ–™', 'å…¶ä»–', 'æœªåˆ†ç±»', 'é€šç”¨', 'documents', 'files', 'misc'
        ])
        
        # å¯»æ‰¾æœ€é€šç”¨çš„æ–‡ä»¶å¤¹
        for priority_name in fallback_list:
            for folder_path in folders:
                if priority_name.lower() in folder_path.lower():
                    self.logger.info(f"ä½¿ç”¨é…ç½®åŒ–å›é€€æ–‡ä»¶å¤¹: {folder_path}")
                    return folder_path
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªé¡¶çº§æ–‡ä»¶å¤¹
        top_level_folders = [f for f in folders if '/' not in f and '\\' not in f]
        if top_level_folders:
            result = top_level_folders[0]
            self.logger.info(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªé¡¶çº§æ–‡ä»¶å¤¹: {result}")
            return result
        
        # æœ€åçš„å›é€€ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ–‡ä»¶å¤¹
        if folders:
            result = folders[0]
            self.logger.info(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ–‡ä»¶å¤¹: {result}")
            return result
        
        return None
    
    def _find_semantic_category_match(self, subject: str) -> Optional[str]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§æ‰¾åˆ°åˆ†ç±»ç±»åˆ«ï¼ˆä¸ä¾èµ–ç°æœ‰æ–‡ä»¶å¤¹ï¼‰"""
        if not self.classification_rules:
            return None
        
        try:
            # è¯»å–é…ç½®æ–‡ä»¶è·å–æœ€æ–°è§„åˆ™
            import yaml
            from pathlib import Path
            
            rules_file = Path("config/classification_rules.yaml")
            if rules_file.exists():
                with open(rules_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                rules = config.get('classification_rules', {})
            else:
                rules = self.classification_rules
                
        except Exception as e:
            self.logger.warning(f"è¯»å–åˆ†ç±»è§„åˆ™å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜ä¸­çš„è§„åˆ™: {e}")
            rules = self.classification_rules
        
        subject_lower = subject.lower()
        threshold = 0.02  # é™ä½é˜ˆå€¼
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åˆ†æ•°
        category_scores = {}
        
        for category_name, category_info in rules.items():
            keywords = category_info.get('keywords', [])
            priority = category_info.get('priority', 99)
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•° - è¿›ä¸€æ­¥æ”¹è¿›ç®—æ³•
            matched_keywords = 0
            total_keyword_score = 0
            high_confidence_matches = 0
            partial_matches = 0
            
            for keyword in keywords:
                keyword_lower = keyword.lower()
                match_score = 0
                
                # 1. å®Œå…¨åŒ¹é…æ£€æŸ¥
                if keyword_lower == subject_lower:
                    match_score = 1.0
                    high_confidence_matches += 1
                # 2. å®Œæ•´åŒ…å«åŒ¹é…
                elif keyword_lower in subject_lower:
                    match_score = 0.9 if len(keyword_lower) >= 4 else 0.7
                    high_confidence_matches += 1
                # 3. åå‘åŒ…å«åŒ¹é…ï¼ˆä¸»ä½“åŒ…å«åœ¨å…³é”®è¯ä¸­ï¼‰
                elif subject_lower in keyword_lower:
                    match_score = 0.8
                    high_confidence_matches += 1
                # 4. éƒ¨åˆ†å•è¯åŒ¹é…
                else:
                    # æ£€æŸ¥å…³é”®è¯çš„å•è¯æ˜¯å¦åœ¨ä¸»ä½“ä¸­
                    keyword_words = set(keyword_lower.split())
                    subject_words = set(subject_lower.split())
                    
                    if keyword_words & subject_words:  # æœ‰äº¤é›†
                        overlap_ratio = len(keyword_words & subject_words) / len(keyword_words)
                        if overlap_ratio >= 0.6:  # 60%ä»¥ä¸Šå•è¯åŒ¹é…
                            match_score = 0.6 * overlap_ratio
                            partial_matches += 1
                        elif overlap_ratio >= 0.3:  # 30%ä»¥ä¸Šå•è¯åŒ¹é…
                            match_score = 0.4 * overlap_ratio
                            partial_matches += 1
                
                if match_score > 0:
                    matched_keywords += 1
                    total_keyword_score += match_score
            
            # å½’ä¸€åŒ–åˆ†æ•°ï¼šåŸºäºåŒ¹é…çš„å…³é”®è¯æ•°é‡å’Œè´¨é‡
            if matched_keywords > 0:
                # åŸºç¡€åˆ†æ•°ï¼šå¹³å‡åŒ¹é…åˆ†æ•°
                base_score = total_keyword_score / len(keywords)
                
                # å¥–åŠ±æœºåˆ¶
                coverage_bonus = min(matched_keywords / len(keywords), 0.4)  # å…³é”®è¯è¦†ç›–å¥–åŠ±
                confidence_bonus = min(high_confidence_matches * 0.15, 0.3)  # é«˜ç½®ä¿¡åº¦å¥–åŠ±
                partial_bonus = min(partial_matches * 0.05, 0.1)  # éƒ¨åˆ†åŒ¹é…å¥–åŠ±
                
                # ç‰¹æ®Šç±»åˆ«ä¼˜å…ˆçº§å¥–åŠ±
                priority_bonus = 0
                if category_name in ['æŠ€æœ¯å¼€å‘', 'ä¸ªäººè´¢åŠ¡', 'å­¦ä¹ æˆé•¿']:
                    priority_bonus = 0.1
                elif category_name in ['é¡¹ç›®ç®¡ç†', 'è¿ç»´ç®¡ç†']:
                    priority_bonus = 0.05
                
                # é•¿å…³é”®è¯å¥–åŠ±ï¼ˆæ›´å…·ä½“çš„å…³é”®è¯ç»™äºˆå¥–åŠ±ï¼‰
                long_keyword_bonus = 0
                for keyword in keywords:
                    if len(keyword) >= 6 and keyword.lower() in subject_lower:
                        long_keyword_bonus += 0.05
                long_keyword_bonus = min(long_keyword_bonus, 0.2)
                
                final_score = base_score + coverage_bonus + confidence_bonus + partial_bonus + priority_bonus + long_keyword_bonus
                
                if final_score >= threshold:
                    category_scores[category_name] = {
                        'score': final_score,
                        'priority': priority,
                        'matched_keywords': matched_keywords,
                        'high_confidence': high_confidence_matches,
                        'partial_matches': partial_matches
                    }
        
        if not category_scores:
            return None
        
        # é€‰æ‹©æœ€ä½³åŒ¹é…ï¼šä¼˜å…ˆæŒ‰åˆ†æ•°ï¼Œç„¶åæŒ‰ä¼˜å…ˆçº§
        best_category_name, best_info = max(
            category_scores.items(),
            key=lambda x: (x[1]['score'], -x[1]['priority'])
        )
        
        # è·å–è¯¥ç±»åˆ«çš„ç¬¬ä¸€ä¸ªç›®æ ‡æ¨¡å¼ä½œä¸ºæ–‡ä»¶å¤¹å
        target_patterns = rules[best_category_name].get('target_patterns', [])
        if target_patterns:
            folder_name = target_patterns[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å¼
            
            self.logger.info(f"è¯­ä¹‰ç±»åˆ«åŒ¹é… - ç±»åˆ«: {best_category_name}, åˆ†æ•°: {best_info['score']:.3f}, ä¼˜å…ˆçº§: {best_info['priority']}, å»ºè®®æ–‡ä»¶å¤¹: {folder_name}")
            return folder_name
        
        return None 